# ğŸ´ Koniara: End-to-End Horse Racing Data Pipeline

This repository contains a modular, GCP-native pipeline for scraping, merging, cleaning, and ingesting horse racing data from the public Homas API into BigQuery. Itâ€™s composed of:

- **Services** (Cloud Run jobs) for batch scraping:  
  - `horse_data_scraper-v2/` ğŸ  
  - `breeder_scraper-v2/` ğŸ„  
  - `jockey_scraper-v2/` ğŸ‡  
  - `trainer_scraper-v2/` ğŸ§‘â€ğŸŒ¾  

- **Functions** for shard management and cleaning:  
  - `merge_shards/` ğŸ”— â€” HTTP Cloud Function to concatenate NDJSON shards  
  - `clean_master/` ğŸ§¹ â€” Cloud Run job to drop malformed JSON & dedupe  

- **Ingestion** (Cloud Run) to load cleaned NDJSON into BigQuery staging & upsert into target tables.

- **Workflows** for orchestration (to be implemented): end-to-end chaining of scrape â†’ merge â†’ clean â†’ ingest.

---

## Table of Contents

1. [Architecture & Flow](#architecture--flow)  
2. [Getting Started](#getting-started)  
3. [Directory Layout](#directory-layout)  
4. [Prerequisites](#prerequisites)  
5. [Deploying Components](#deploying-components)  
6. [Testing & Monitoring](#testing-monitoring)  
7. [Next Steps](#next-steps)  

---

## Architecture & Flow

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   scrape   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   merge   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   clean   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   ingest   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Client â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚ Scraperâ”‚ â”€â”€â”€â”€â”€â”€â”€> â”‚ merge-  â”‚ â”€â”€â”€â”€â”€â”€â”€> â”‚ clean-  â”‚ â”€â”€â”€â”€â”€â”€â”€> â”‚ Ingest â”‚
â”‚        â”‚            â”‚ (Run)  â”‚          â”‚ shards  â”‚          â”‚ master  â”‚          â”‚ (Run)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
````

1. **Scrape** in small batches (1,000 IDs) with 10Ã— concurrency, halt after 10Ã— 404s.
2. **Merge** per-entity shards into one NDJSON (`mergeShards`).
3. **Clean** the merged file: drop malformed lines & duplicates (`cleanMaster`).
4. **Ingest** cleaned NDJSON into BigQuery (stagingâ†’MERGE).
5. (Planned) **Orchestrate** via Cloud Workflows & Cloud Scheduler.

---

## Getting Started

Clone the repo and choose a component:

```bash
git clone https://github.com/elomack/koniara.git
cd koniara
```

Then follow that componentâ€™s `README.md` under `services/â€¦` or `functions/â€¦` for detailed build & deploy instructions.

---

## Directory Layout

```
koniara/
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ horse_data_scraper-v2/
â”‚   â”œâ”€â”€ breeder_scraper-v2/
â”‚   â”œâ”€â”€ jockey_scraper-v2/
â”‚   â”œâ”€â”€ trainer_scraper-v2/
â”‚   â””â”€â”€ data_ingestion_service/
â”‚       â””â”€â”€ src/
â”‚           â”œâ”€â”€ index.js         # ingestion logic
â”‚           â””â”€â”€ cleanMaster.js   # NDJSON cleaner
â”œâ”€â”€ functions/
â”‚   â””â”€â”€ merge_shards/
â”‚       â””â”€â”€ index.js             # mergeShards Cloud Function
â”œâ”€â”€ workflows/
â”‚   â””â”€â”€ horse-pipeline-full.yaml # (planned) Cloud Workflow definition
â””â”€â”€ README.md                    # â† you are here
```

---

## Prerequisites

* **GCP project** with:

  * Cloud Run, Cloud Functions, Cloud Workflows APIs enabled
  * IAM roles: Cloud Run Admin, Cloud Functions Developer, Storage Admin, BigQuery Admin
* **Node.js 20.x** & **Docker**
* **gcloud CLI** authenticated to your project

---

## Deploying Components

Each component has its own `README.md`. In general:

1. **Scraper Services**

   ```bash
   cd services/horse_data_scraper-v2
   npm install
   docker build -t gcr.io/$PROJECT_ID/horse-scraper:v1 .
   docker push gcr.io/$PROJECT_ID/horse-scraper:v1
   gcloud run jobs deploy horse-scraper-job \
     --image=gcr.io/$PROJECT_ID/horse-scraper:v1 \
     ... (flags vary per service)
   ```

2. **mergeShards Function**

   ```bash
   cd functions/merge_shards
   npm install
   gcloud functions deploy mergeShards \
     --region=europe-central2 \
     --runtime=nodejs20 \
     --trigger-http \
     --entry-point=mergeShards \
     --set-env-vars=BUCKET_NAME=$BUCKET
   ```

3. **cleanMaster Job**

   ```bash
   cd services/data_ingestion_service
   npm install
   docker build -t gcr.io/$PROJECT_ID/clean-master-job:v1 .
   docker push gcr.io/$PROJECT_ID/clean-master-job:v1
   gcloud run jobs deploy clean-master-job ...
   ```

4. **BigQuery Ingestion**

   * See `services/data_ingestion_service/src/index.js` for staging & MERGE logic.

---

## Testing & Monitoring

* **Local**: run each `index.js` with test IDs.
* **Cloud Run & Functions** logs:

  ```bash
  gcloud run jobs logs read <job-name> --region=...  
  gcloud functions logs read mergeShards --region=...
  ```
* **Logs-based metrics**: count â€œâŒâ€ errors or â€œâš ï¸ Missâ€ entries.
* **Alerts**: configure in Cloud Monitoring.

---

## Next Steps

1. **Build & validate** the `cleanMaster` and ingestion jobs end-to-end.
2. **Create mini-Workflows** for each data type: scraperâ†’mergeâ†’cleanâ†’ingest.
3. **Compose full orchestrator** in `workflows/horse-pipeline-full.yaml`.
4. **Schedule** via Cloud Scheduler.
5. **Cleanup**: enable shard deletion, finalize IAM, add alerts.

> **Guiding principle**: â€œLock down your core transforms first, then compose the full Workflow once each stage is hardened.â€

```
