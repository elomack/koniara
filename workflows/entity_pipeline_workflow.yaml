# entity_pipeline_workflow.yaml
# Sub-workflow: loop over batches and invoke a single Cloud Run Job per batch

main:
  params: [payload]
  steps:
    # [0] Initialization of variables
    - init_vars:
        assign:
          - projectId: "horse-predictor-v2"
          - region:    "europe-central2"
          - batches:   ${int(payload.batches)}
          - batchSize: ${int(payload.batchSize)}
          - startId:   ${int(payload.startId)}
          - serviceName: ${payload.serviceName}
          - prefix:    ${payload.prefix}

    # [1] Log start
    - log_init:
        call: sys.log
        args:
          text: >-
            ${"ğŸ”§ Starting pipeline: service=" + serviceName
              + ", batches=" + string(batches)
              + ", batchSize=" + string(batchSize)
              + ", startId=" + string(startId)}
          severity: INFO

    # [2] Batch loop
    - batchLoop:
        for:
          value: i
          range:
            - 0
            - ${batches - 1}
          steps:
            # [2a] Log iteration
            - log_iteration:
                call: sys.log
                args:
                  text: >-
                    ${"â¡ï¸ Batch " + string(i + 1) + "/" + string(batches)
                      + " (startId=" + string(startId + i * batchSize)
                      + ", batchSize=" + string(batchSize) + ")"}
                  severity: INFO

            # [2b] Run scraper job
            - run_batch_job:
                call: googleapis.run.v1.namespaces.jobs.run
                args:
                  name: ${"namespaces/" + projectId + "/jobs/" + serviceName}
                  location: ${region}
                  body:
                    overrides:
                      containerOverrides:
                        - args:
                            - ${string(startId + i * batchSize)}
                            - ${string(batchSize)}
                result: jobResp

            # [2c] Log job invocation
            - log_job_resp:
                call: sys.log
                args:
                  text: >-
                    ${"âœ… Job invoked: " + jobResp.metadata.name}
                  severity: INFO

    # [3] Merge shards for this prefix
    - merge:
        call: http.post
        args:
          url: ${payload.mergeUrl}
          auth:
            type: OIDC
          body:
            prefix: ${prefix}
            outputPrefix: ${prefix}
            pattern: '^shard_.*\.ndjson$'
        result: mergeResp

        # [4] Clean and dedupe merged masterfile
    - clean:
        call: http.post
        args:
          url: ${payload.cleanUrl}
          auth:
            type: OIDC
          body:
            prefix: ${prefix}
        result: cleanResp

    # [5] Ingest cleaned data into BigQuery
    - ingest:
        call: http.post
        args:
          url: ${payload.ingestUrl}
          auth:
            type: OIDC
          body:
            prefix: ${prefix}
        result: ingestResp

    # [6] Completion log

    - log_done:
        call: sys.log
        args:
          text: >-
            ${"ğŸ Completed " + string(batches) + " batches for " + serviceName}
          severity: INFO

    # [5] Return batches processed
    - return:
        return: ${batches}
